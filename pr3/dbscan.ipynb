{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import math\n",
    "import re\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import sklearn.metrics.pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the dataset\n",
    "with open(\"train.dat\", \"r\") as tr:\n",
    "    train_data = tr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "380\n",
      "718 1 20883 2 759 3 16 1 18923 1 2402 1 17 1 18925 1 18926 2 2281 1 4027 2 1435 1 9150 1 9033 1 1359 1 6284 1 1992 1 407 1 443 1 18210 5 562 1 166 2 924 1 169 7 926 1 18218 1 61 1 1126 1 1360 5 29 1 1046 2 3422 1 1244 2 66 2 1362 1 4073 1 2453 1 3506 1 1364 3 1283 1 69 1 1366 1 9160 1 3464 1 1367 4 2097 1 2579 1 1886 1 8 1 332 2 12205 1 2979 3 619 1 692 3 11438 1 577 1 30 1 11917 1 15276 1 3113 2 2105 1 1371 1 3038 2 9050 1 6063 1 78 1 1378 3 1496 1 142 1 421 3 540 2 264 1 428 1 384 2 10953 1 10638 4 946 1 785 2 21683 2 21684 1 21685 1 21686 1 10797 1 21687 1 20679 1 2033 1 21688 1 1026 2 18916 1 18917 1 989 1 18918 1 3045 4 2155 2 1382 1 2157 1 1149 3 4093 5 6911 2 117 1 2995 2 1789 3 154 1 634 2 393 1 555 1\n",
      "\n",
      "358 3 759 2 1070 1 3414 2 2127 1 57 2 1633 1 2605 1 1075 1 2281 1 3416 1 1391 3 2680 1 3256 1 5670 1 5671 1 4547 2 5672 1 5673 1 204 1 5674 1 5675 1 5676 1 5677 1 523 1 5678 1 5679 1 362 1 763 4 844 1 682 2 369 2 289 1 61 1 1403 1 1242 1 3025 1 3109 1 2057 1 1409 1 5605 1 1961 2 1169 2 5680 1 5681 1 5682 1 5 1 5683 1 6 2 5684 2 215 1 134 1 5685 1 5686 1 135 1 9 5 532 1 5687 2 4797 1 5688 1 372 1 5689 3 534 2 616 1 375 1 539 1 4001 1 778 1 1411 1 70 2 779 1 2700 1 1416 1 1570 1 1096 1 5690 1 5691 1 5692 1 1894 1 1976 1 5693 1 5694 1 2787 1 5695 1 306 1 5696 1 148 1 942 1 384 1 387 1 1020 1 869 1 1422 2 83 1 1506 3 85 1 2155 1 3442 1 1742 1 5460 1 4737 2 150 1 3848 1 4499 1 156 3 391 1 392 1 474 1 393 1 556 1 559 1 1111 1 958 1 2080 2 1271 1 3053 2 1119 1 94 2 3057 1 3377 1 162 2 405 2 560 1 167 1 169 1 962 1 487 1 24 3 2016 1 1126 1 28 1 5321 1 29 1 1362 1 3623 1 3463 1 1843 1 330 1 412 1 413 2 332 1 414 1 333 1 2979 1 654 1 658 1 30 1 975 2 2023 1 1135 1 1056 1 4521 2 2661 1 2743 1 1537 1 340 1 421 1 503 1 107 1 262 2 263 1 428 1 662 1 267 1 348 1 188 1 2033 1 5102 1 1701 1 1143 1 1380 2 49 2 3482 1 1862 2 4292 1 2591 2 1389 1 5664 4 5665 3 5666 1 5667 1 5668 1 1789 1 351 1 5669 1 190 1 353 1 670 1 356 1 195 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print len(train_data[10].split())\n",
    "print len(train_data[1].split())\n",
    "\n",
    "print train_data[10]\n",
    "print train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csr_read(fname, ftype=\"csr\", nidx=1):\n",
    "    r\"\"\" \n",
    "        Read CSR matrix from a text file. \n",
    "        \n",
    "        \\param fname File name for CSR/CLU matrix\n",
    "        \\param ftype Input format. Acceptable formats are:\n",
    "            - csr - Compressed sparse row\n",
    "            - clu - Cluto format, i.e., CSR + header row with \"nrows ncols nnz\"\n",
    "        \\param nidx Indexing type in CSR file. What does numbering of feature IDs start with?\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if ftype == \"clu\":\n",
    "        p = lines[0].split()\n",
    "        nrows = int(p[0])\n",
    "        ncols = int(p[1])\n",
    "        nnz = long(p[2])\n",
    "        lines = lines[1:]\n",
    "        assert(len(lines) == nrows)\n",
    "    elif ftype == \"csr\":\n",
    "        nrows = len(lines)\n",
    "        ncols = 0 \n",
    "        nnz = 0 \n",
    "        for i in xrange(nrows):\n",
    "            p = lines[i].split()\n",
    "            if len(p) % 2 != 0:\n",
    "                raise ValueError(\"Invalid CSR matrix. Row %d contains %d numbers.\" % (i, len(p)))\n",
    "            nnz += len(p)/2\n",
    "            for j in xrange(0, len(p), 2): \n",
    "                cid = int(p[j]) - nidx\n",
    "                if cid+1 > ncols:\n",
    "                    ncols = cid+1\n",
    "    else:\n",
    "        raise ValueError(\"Invalid sparse matrix ftype '%s'.\" % ftype)\n",
    "    val = np.zeros(nnz, dtype=np.float)\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.long)\n",
    "    n = 0 \n",
    "    for i in xrange(nrows):\n",
    "        p = lines[i].split()\n",
    "        for j in xrange(0, len(p), 2): \n",
    "            ind[n] = int(p[j]) - nidx\n",
    "            val[n] = float(p[j+1])\n",
    "            n += 1\n",
    "        ptr[i+1] = n \n",
    "    \n",
    "    assert(n == nnz)\n",
    "    \n",
    "    return csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.float)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(val,ind,ptr,(rows, cols)) = csr_read(\"train.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix= csr_read(\"train.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8580L, 126355L)\n",
      "(8580L, 126355L)\n"
     ]
    }
   ],
   "source": [
    "print matrix.toarray().shape\n",
    "print matrix.todense().shape\n",
    "\n",
    "M = matrix.todense()\n",
    "distances = sklearn.metrics.pairwise.pairwise_distances(M,M)\n",
    "print distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mtx = csr_matrix((3, 4), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3L, 3L)\n"
     ]
    }
   ],
   "source": [
    "print mtx.todense().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 2)\t2\n",
      "  (1, 2)\t3\n",
      "  (2, 0)\t4\n",
      "  (2, 1)\t5\n",
      "  (2, 2)\t6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 2],\n",
       "        [0, 0, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "indices = np.array([0, 2, 2, 0, 1, 2])\n",
    "indptr = np.array([0, 2, 3, 6])\n",
    "mtx = csr_matrix((data, indices, indptr), shape=(3, 3))\n",
    "print mtx\n",
    "mtx.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNCLASSIFIED = False\n",
    "NOISE = None\n",
    "\n",
    "def _dist(p,q):\n",
    "\treturn math.sqrt(np.power(p-q,2).sum())\n",
    "\n",
    "def _eps_neighborhood(p,q,eps):\n",
    "\treturn _dist(p,q) < eps\n",
    "\n",
    "\n",
    "#calculate distance to all points(columns) in the matrix and add to seed if it is < eps\n",
    "def _region_query(m, point_id, eps):\n",
    "    n_points = m.shape[1]\n",
    "    seeds = []\n",
    "    for i in range(0, n_points):\n",
    "        if _eps_neighborhood(m[:,point_id], m[:,i], eps):\n",
    "            seeds.append(i)\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _expand_cluster(m, classifications, point_id, cluster_id, eps, min_points):\n",
    "    seeds = _region_query(m, point_id, eps)\n",
    "    if len(seeds) < min_points:\n",
    "        classifications[point_id] = NOISE\n",
    "        return False\n",
    "    else:\n",
    "        classifications[point_id] = cluster_id\n",
    "        for seed_id in seeds:\n",
    "            classifications[seed_id] = cluster_id\n",
    "            \n",
    "        while len(seeds) > 0:\n",
    "            current_point = seeds[0]\n",
    "            results = _region_query(m, current_point, eps)\n",
    "            if len(results) >= min_points:\n",
    "                for i in range(0, len(results)):\n",
    "                    result_point = results[i]\n",
    "                    if classifications[result_point] == UNCLASSIFIED or \\\n",
    "                       classifications[result_point] == NOISE:\n",
    "                        if classifications[result_point] == UNCLASSIFIED:\n",
    "                            seeds.append(result_point)\n",
    "                        classifications[result_point] = cluster_id\n",
    "            seeds = seeds[1:]\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dbscan(m, eps, min_points):\n",
    "    \"\"\"Implementation of Density Based Spatial Clustering of Applications with Noise\n",
    "    See https://en.wikipedia.org/wiki/DBSCAN\n",
    "    \n",
    "    scikit-learn probably has a better implementation\n",
    "    \n",
    "    Uses Euclidean Distance as the measure\n",
    "    \n",
    "    Inputs:\n",
    "    m - A matrix whose columns are feature vectors(each column represents a record)\n",
    "    eps - Maximum distance two points can be to be regionally related\n",
    "    min_points - The minimum number of points to make a cluster\n",
    "    \n",
    "    Outputs:\n",
    "    An array with either a cluster id number or dbscan.NOISE (None) for each\n",
    "    column vector in m.\n",
    "    \"\"\"\n",
    "    cluster_id = 1\n",
    "    \n",
    "    #this will have the number of records.\n",
    "    n_points = m.shape[1] \n",
    "    print n_points\n",
    "    \n",
    "    #Assigning default classifications to all records.\n",
    "    classifications = [UNCLASSIFIED] * n_points\n",
    "    print classifications\n",
    "    \n",
    "    #loop over number of records\n",
    "    for point_id in range(0, n_points):\n",
    "        #get the current record\n",
    "        point = m[:,point_id]\n",
    "        print point.shape\n",
    "        if classifications[point_id] == UNCLASSIFIED:\n",
    "            if _expand_cluster(m, classifications, point_id, cluster_id, eps, min_points):\n",
    "                cluster_id = cluster_id + 1\n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_dbscan():\n",
    "    m = np.matrix('1 1.2 0.8 3.7 3.9 3.6 10; 1.1 0.8 1 4 3.9 4.1 10')\n",
    "    eps = 0.5\n",
    "    min_points = 2\n",
    "    print \"sudheer\"\n",
    "    print dbscan(m, eps, min_points)\n",
    "    assert dbscan(m, eps, min_points) == [1, 1, 1, 2, 2, 2, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudheer\n",
      "[1, 1, 1, 2, 2, 2, None]\n"
     ]
    }
   ],
   "source": [
    "test_dbscan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudheer\n",
      "9\n",
      "sammeta\n",
      "9\n",
      "[False, False, False, False, False, False, False, False, False]\n",
      "(2L, 1L)\n",
      "(2L, 1L)\n",
      "(2L, 1L)\n",
      "(2L, 1L)\n",
      "(2L, 1L)\n",
      "(2L, 1L)\n",
      "(2L, 1L)\n",
      "(2L, 1L)\n",
      "(2L, 1L)\n",
      "[None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "m = np.matrix('1 1.2 0.8 3.7 3.9 3.6 10; 1.1 0.8 1 4 3.9 4.1 10')\n",
    "\n",
    "m = np.matrix('1 2 3 4 5 6 7 8 9;11 22 33 44 55 66 77 88 99')\n",
    "eps = 0.5\n",
    "min_points = 2\n",
    "print \"sudheer\"\n",
    "print m.shape[1]\n",
    "print \"sammeta\"\n",
    "print dbscan(m, eps, min_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.    1.1]\n",
      " [  1.2   0.8]\n",
      " [  0.8   1. ]\n",
      " [  3.7   4. ]\n",
      " [  3.9   3.9]\n",
      " [  3.6   4.1]\n",
      " [ 10.   10. ]]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "[6]\n",
      "[6]\n",
      "[ 1.  1.  1.  2.  2.  2.  0.]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "m = np.matrix('1 1.2 0.8 3.7 3.9 3.6 10; 1.1 0.8 1 4 3.9 4.1 10; 1.1 0.9 1 7 3.9 4.1 10')\n",
    "m = np.matrix('1 1.2 0.8 3.7 3.9 3.6 10; 1.1 0.8 1 4 3.9 4.1 10').T\n",
    "print m\n",
    "rec_count = m.shape[0]\n",
    "\n",
    "\n",
    "#Calculating distance between each pair of points\n",
    "dist_matrix= np.zeros((rec_count,rec_count))\n",
    "for row in range(0, rec_count):\n",
    "    current =  m[row, :]\n",
    "    #dist_matrix.append(distance_from_point(current, row, m, dist_matrix))\n",
    "    distance_from_point(current, row, m, dist_matrix)\n",
    "\n",
    "    \n",
    "#print dist_matrix    \n",
    "\n",
    "#Count core points\n",
    "core_point_indexes= []\n",
    "not_core_point_indexes = []\n",
    "eps = 0.5\n",
    "min_points = 2\n",
    "\n",
    "\n",
    "#using index notation(code below is more readable.)\n",
    "# for row in range(0, rec_count):\n",
    "#     print dist_matrix[row]\n",
    "#     print dist_matrix[row][np.where( dist_matrix[row] <= eps )].size\n",
    "#     if dist_matrix[row][np.where( dist_matrix[row] <= eps )].size > min_points:\n",
    "#         core_point_indexes.append(row)\n",
    "\n",
    "# print core_point_indexes\n",
    "\n",
    "\n",
    "#Identifying core and not core points.\n",
    "row = 0\n",
    "for rec in dist_matrix:\n",
    "    #print rec[np.where( rec <= eps )].size\n",
    "    if rec[np.where( rec <= eps )].size > min_points:\n",
    "        core_point_indexes.append(row)\n",
    "    else:\n",
    "        not_core_point_indexes.append(row)\n",
    "    row += 1\n",
    "\n",
    "print core_point_indexes\n",
    "print not_core_point_indexes\n",
    "\n",
    "\n",
    "\n",
    "#Identifying border points, its closest core point and noise points\n",
    "border_point_map = {}\n",
    "noise_point_index= []\n",
    "\n",
    "for i in not_core_point_indexes:\n",
    "    min = 0\n",
    "    for j in core_point_indexes:\n",
    "        if dist_matrix[i][j] <= eps:\n",
    "            if (min > 0 and dist_matrix[i][j] < min) or min == 0:\n",
    "                border_point_map[i] = j\n",
    "                min = dist_matrix[i][j]\n",
    "    if min == 0:\n",
    "        noise_point_index.append(i)\n",
    "\n",
    "print noise_point_index\n",
    "\n",
    "\n",
    "cluster= np.zeros(rec_count)\n",
    "cluster_count = 0\n",
    "for i in core_point_indexes:\n",
    "    if cluster[i] == 0:\n",
    "        cluster_count +=1\n",
    "        cluster[i] = cluster_count\n",
    "    for j in core_point_indexes:\n",
    "        if cluster[j] == 0  and dist_matrix[i][j] <= eps:\n",
    "            cluster[j] = cluster[i]\n",
    "    \n",
    "print cluster\n",
    "\n",
    "#Clustering the core points\n",
    "\n",
    "#Creating not oc\n",
    "# border_point_indexes = []\n",
    "# core_size = len(core_point_indexes)\n",
    "# j=0\n",
    "# for i in range(0, rec_count):\n",
    "#         print \"j is\", j\n",
    "#         if i < core_point_indexes[j]:\n",
    "#             border_point_indexes.append(i)\n",
    "#         elif i == core_point_indexes[j]:\n",
    "#             continue\n",
    "#         else:\n",
    "#             if j+1 < core_size:\n",
    "#                 j+=1\n",
    "#                 if i < core_point_indexes[j]:\n",
    "#                     border_point_indexes.append(i)\n",
    "#             else:\n",
    "#                 for a in range(core_point_indexes[j]+1, rec_count):\n",
    "#                     border_point_indexes.append(a)\n",
    "#                 break\n",
    "\n",
    "print border_point_indexes            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate distance to all points(columns) in the matrix and add to seed if it is < eps\n",
    "#def distance_from_point(current, m):\n",
    "#     total_records = m.shape[0]\n",
    "#     distances = []\n",
    "#     for i in range(0, total_records):\n",
    "#         d = math.sqrt(np.power(current-m[i,:],2).sum())\n",
    "#         distances.append(d)\n",
    "#     return distances\n",
    "\n",
    "\n",
    "def distance_from_point(current, row, m, dist_matrix):\n",
    "    total_records = m.shape[0]\n",
    "    for i in range(row+1, total_records):\n",
    "        d = math.sqrt(np.power(current-m[i,:],2).sum())\n",
    "        dist_matrix[row][i] = dist_matrix[i][row] = d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j is 0\n",
      "j is 0\n",
      "j is 1\n",
      "2 sudheer\n",
      "j is 1\n",
      "3 sudheer\n",
      "j is 1\n",
      "4 sudheer\n",
      "j is 1\n",
      "5 sudheer\n",
      "j is 1\n",
      "6 sudheer\n",
      "j is 1\n",
      "7 sudheer\n",
      "j is 1\n",
      "8 sudheer\n",
      "j is 1\n",
      "[1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# x = np.arange(9.).reshape(3, 3)\n",
    "# print x\n",
    "\n",
    "# print np.where( x > 5 )\n",
    "\n",
    "# m = np.matrix('1 2 3 4 5 6 10; 1 12 13 14 15 16 20; 21 22 23 24 25 26 30')\n",
    "\n",
    "# print m[np.where( m[0] > 1.0 )].size               # Note: result is 1D.\n",
    "\n",
    "# print np.where(x < 5, x, -1)               # Note: broadcasting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Count border points\n",
    "rec_count=10\n",
    "core_point_indexes = [0,9]\n",
    "border_point_indexes = []\n",
    "size = len(core_point_indexes)\n",
    "j=0\n",
    "for i in range(0, rec_count):\n",
    "        print \"j is\", j\n",
    "        if i < core_point_indexes[j]:\n",
    "            print i , \"sudheer\"\n",
    "            border_point_indexes.append(i)\n",
    "        elif i == core_point_indexes[j]:\n",
    "            continue\n",
    "        else:\n",
    "            if j+1 < size:\n",
    "                j+=1\n",
    "                if i < core_point_indexes[j]:\n",
    "                    border_point_indexes.append(i)\n",
    "            else:\n",
    "                for a in range(core_point_indexes[j]+1, rec_count):\n",
    "                    border_point_indexes.append(a)\n",
    "                break\n",
    "\n",
    "print border_point_indexes            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
